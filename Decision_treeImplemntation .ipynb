{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math as m\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load of our Iris data \n",
    "iris= datasets.load_iris()\n",
    "print(iris)\n",
    "df = pd.DataFrame(iris.data)  # Giving it the data Frame\n",
    "Y = pd.DataFrame(iris.target)  #  our output of the iris data  \n",
    "df.columns = [\"sl\", \"sw\", 'pl', 'pw']\n",
    "features   = [\"sl\", \"sw\", 'pl', 'pw']\n",
    "# splitting of data features into x_train y_train x_test y_test \n",
    "x_train, x_test, y_train, y_test = train_test_split(df,Y,random_state = 1)\n",
    "x_train=x_train.reset_index(drop=True)\n",
    "y_train=y_train.reset_index(drop=True)\n",
    "x_test=x_test.reset_index(drop=True)\n",
    "y_test=y_test.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node is  made to make the tree for the given data \n",
    "class node:\n",
    "    def __init__(self,f=None,fv=None,gr=-1,en=-1,val=-1):\n",
    "        self.entropy=en      # Present Entropy\n",
    "        self.f_to_split=f    # Feature about which the current node will be splitted (if not a leaf node)\n",
    "        self.feature_val=fv  # The label of the feature about which the data was split to get the present node \n",
    "        self.gain_ratio=gr   # Gain Ratio by which it is gonna be splitted\n",
    "        self.children = []   # Initialising children as an empty list\n",
    "        self.midF = val      # data to store mid value  \n",
    "        self.array = []      # array to store zeros ones and twos classes of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is function for the partition of the continues data so that we can the best possible mid value so that we can spilt our data in  to two best features   \n",
    "def Partition(array,midPoint,Y,k):\n",
    "    # ans1 store the data for the values less tha mid \n",
    "    # ans2 store the values for the data more than or equal to mid\n",
    "    ans1= np.zeros(3,dtype=int)\n",
    "    ans2= np.zeros(3,dtype=int)\n",
    "    Y = np.array(Y)\n",
    "    # loop over the array(with bestfeature) and Y is the output for our bestFeature \n",
    "    for i in range(array.shape[0]):\n",
    "        if array[i] < midPoint:\n",
    "            if Y[i] == 0:\n",
    "                ans1[0]+=1\n",
    "            elif Y[i] == 1:\n",
    "                ans1[1]+=1\n",
    "            elif Y[i]==2:\n",
    "                ans1[2]+=1\n",
    "        else:\n",
    "            if Y[i] == 0:\n",
    "                ans2[0]+=1\n",
    "            elif Y[i] == 1:\n",
    "                ans2[1]+=1\n",
    "            elif Y[i]==2:\n",
    "                ans2[2]+=1\n",
    "    return ans1,ans2            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_Gain(ans):\n",
    "    value = ans[0] + ans[1] +ans[2]\n",
    "    info=0\n",
    "    for i in range(3):\n",
    "        if value == 0:\n",
    "            X=0\n",
    "        if value!=0:\n",
    "            X = ans[i]/value\n",
    "        if X == 0:\n",
    "            info += 0\n",
    "        if X!=0:\n",
    "            info += (-1)*(X * m.log(X))\n",
    "    return info,value    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This the function which will give us the InfoGain or Entropy  \n",
    "def Info_Gain(X,Y,selFtre):\n",
    "    # array with bestFeature \n",
    "    array = X[selFtre]\n",
    "    maxGain = -1   # maxgain is the data which will us the best infoGain or entropy   \n",
    "    mid = -1       \n",
    "    for i in range(array.shape[0]-1):\n",
    "        \n",
    "        #################################################################################################################\n",
    "        \n",
    "        # midPoint cantain the mid value to consecuitive data points  \n",
    "        midPoint = (array[i] + array[i+1] ) / 2\n",
    "        # ans1  and ans2 contain the output divided on the bases of our midpoint\n",
    "        ans1,ans2 = Partition(array,midPoint,Y,i)\n",
    "        # ans3  cantain classes  ouput on the bases of which ans1 and ans2 is created \n",
    "        ans3 = ans1+ans2\n",
    "\n",
    "        ###################################################################################################################\n",
    "        #info_gain is the formula applied on basis of which entropy is given to us  \n",
    "        # value1 value2 value3  contain the number on which our data is divided\n",
    "        InfoGain1,value1 = info_Gain(ans1)\n",
    "        InfoGain2,value2 = info_Gain(ans2)\n",
    "        InfoGain3,value3 = info_Gain(ans3)\n",
    "        ###################################################################################################################\n",
    "        # maxG and value is the formula applied to caluclate the macGain\n",
    "        value = ((value1/value3)*InfoGain1) + ((value2/value3)*InfoGain2)\n",
    "        maxG = InfoGain3 - value\n",
    "        ###################################################################################################################\n",
    "        \n",
    "        # To get the maxGain due to continuous values in features \n",
    "        if maxGain <= maxG:\n",
    "            maxGain = maxG\n",
    "            mid = midPoint\n",
    "            valueArray = np.array([value1,value2,value3])   \n",
    "        \n",
    "        \n",
    "    return maxGain , valueArray , mid \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the formula to calculate the splitInfo to calulate the gain ratio\n",
    "def Split_Info(X,Y,valueArray):\n",
    "    x = valueArray[0] / valueArray[2]\n",
    "    y = valueArray[1] / valueArray[2]\n",
    "    ans = ((-x) * m.log(x)) + ((-y) * m.log(y))\n",
    "    return ans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this the Fuction where we calculate the gain ratio \n",
    "def gain(X,Y,selFtre):\n",
    "    infoGain,valueArray, mid  = Info_Gain(X,Y,selFtre)\n",
    "    splitInfo = Split_Info(X,Y,valueArray)\n",
    "    Gain = infoGain /splitInfo\n",
    "    return Gain,mid,infoGain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This the main fiunction where we get our decision tree\n",
    "def decision_tree(X,Y,features,level,Entropy,root,mid):\n",
    "    # This is our Base case where we reach to our base or leaf node \n",
    "    m = set(Y[0])\n",
    "    if len(m)==1 and (list(m)[0] == 1 or list(m)[0] == 2  or list(m)[0] == 0):  # base case if node becomes pure \n",
    "\n",
    "        # This is the place where we store our data into the node \n",
    "        root.midF  =  mid\n",
    "        root.array.append(len(Y[Y[0]==0]))\n",
    "        root.array.append(len(Y[Y[0]==1]))\n",
    "        root.array.append(len(Y[Y[0]==2]))\n",
    "        root.entropy    = Entropy\n",
    "\n",
    "        # This the place where we print our node level wise \n",
    "        print(\"Level = \" + str(level))\n",
    "        print(\"Count of  0 = \" + str(len(Y[Y[0]==0])))\n",
    "        print(\"Count of  1 = \" + str(len(Y[Y[0]==1])))\n",
    "        print(\"Count of  2 = \" + str(len(Y[Y[0]==2])))\n",
    "        print(\"Current Entropy = \" + str(Entropy))\n",
    "        print(\"Reached the LEAF NODE\")\n",
    "        print()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    if len(features) == 0: \n",
    "        # This is the place where we store our data into the node \n",
    "        \n",
    "        root.midF  =  mid\n",
    "        root.array.append(len(Y[Y[0]==0]))\n",
    "        root.array.append(len(Y[Y[0]==1]))\n",
    "        root.array.append(len(Y[Y[0]==2]))\n",
    "        \n",
    "        \n",
    "         # This the place where we print our node level wise \n",
    "        print(\"Level = \" + str(level))\n",
    "        print(\"Count of  0 = \" + str(len(Y[Y[0]==0])))\n",
    "        print(\"Count of  1 = \" + str(len(Y[Y[0]==1])))\n",
    "        print(\"Count of  2 = \" + str(len(Y[Y[0]==2])))\n",
    "        print(\"Current Entropy = \" + str(Entropy))\n",
    "        print(\"Reached the LEAF NODE\")\n",
    "        print()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # place were we define our variable \n",
    "    \n",
    "    maxGain = 0  # Is the variable which will get us the best gain ratio of the particular feature\n",
    "    mid = 0      # It is varaible which will give us the value on which we to split the our feature into two part\n",
    "    bestFeature = \"\"   # variable to get us the betsFeature   \n",
    "    Entropy=0      # variable to get us the entropy \n",
    "    \n",
    "    # loop on our feature list to get our bestfeature \n",
    "    \n",
    "    for f in features:\n",
    "        # ans contain all the value Gainratio midvalue and the Entropy for the particular feature \n",
    "        ans = gain(X,Y,f)\n",
    "        # we compare the Maxgain to get ou best gain ratio midvalue and entropy\n",
    "        \n",
    "        if maxGain <= ans[0]:\n",
    "            maxGain = ans[0]\n",
    "            mid = ans[1]\n",
    "            bestFeature = f\n",
    "            Entropy = ans[2]\n",
    "    \n",
    "    # Place we store our data into the node  \n",
    "    root.entropy    = Entropy\n",
    "    root.f_to_split = bestFeature\n",
    "    root.midF       = mid\n",
    "    root.children.append(node(fv=bestFeature))\n",
    "    root.array.append(len(Y[Y[0]==0]))\n",
    "    root.array.append(len(Y[Y[0]==1]))\n",
    "    root.array.append(len(Y[Y[0]==2]))\n",
    "                    \n",
    "    # Place where we print our data  \n",
    "    print(\"Level = \" + str(level))\n",
    "    print(\"Count of  0 = \" + str(len(Y[Y[0]==0])))\n",
    "    print(\"Count of  1 = \" + str(len(Y[Y[0]==1])))\n",
    "    print(\"Count of  2 = \" + str(len(Y[Y[0]==2])))\n",
    "    print(\"Current Entropy is = \" + str(Entropy))\n",
    "    print(\"Splitting on feature \" + str(bestFeature)  + \" with gain ratio = \" + str(maxGain))\n",
    "    print()\n",
    "    \n",
    "    # PLace where we split our data on the basis of bestFeature so that we can pass our data to the next level \n",
    "    X1 = X[X[bestFeature]<mid]\n",
    "    Y1 = Y.iloc[X1.index]\n",
    "    X1 = X1.reset_index(drop=True)\n",
    "    Y1 = Y1.reset_index(drop=True)\n",
    "    X1 = X1.drop([bestFeature],axis=1)\n",
    "\n",
    "    # PLace where we split our data on the basis of bestFeature so that we can pass our data to the next level \n",
    "    X2 = X[X[bestFeature]>=mid]\n",
    "    Y2 = Y.iloc[X2.index]\n",
    "    X2 = X2.reset_index(drop=True)\n",
    "    Y2 = Y2.reset_index(drop=True)\n",
    "    X2 = X2.drop([bestFeature],axis=1)\n",
    "\n",
    "    features.remove(bestFeature) # Here we remove the feature which we have used \n",
    "    \n",
    "    decision_tree(X1,Y1,features,level+1,Entropy,root.children[0],mid) # We call our feature to get us the split on our next featsure  \n",
    "    \n",
    "    root.children.append(node(fv=bestFeature)) # here we append new node in our children so that we call save new data for parent node \n",
    "    \n",
    "    decision_tree(X2,Y2,features,level+1,Entropy,root.children[1],mid) # We call our feature to get us the split on our next featsure  \n",
    "    \n",
    "    return \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we get the our root node or our first node \n",
    "root = node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this the function to predict our output fo X_test input \n",
    "\n",
    "def predict(X,root,Y,decide):\n",
    "    \n",
    "    # this the base case were we reach to leaf node and decide our output \n",
    "    if len(root.children) == 0:\n",
    "\n",
    "        bstFtre = root.feature_val\n",
    "        mid = root.midF\n",
    "        \n",
    "        # if our decide is equal to zero then we consider pur left node \n",
    "        if decide == 0:\n",
    "            count1 = max(root.array)  # <mid  count1 contain cvalue with maximum in  class \n",
    "            X1 = X[X[bstFtre]<mid]\n",
    "            for j in range(3):\n",
    "                if count1 == root.array[j]:\n",
    "                    Y.iloc[X1.index] = j   \n",
    "        \n",
    "        # other wise if our decoide is equla to one then we consider right node              \n",
    "        else:\n",
    "            count2 = max(root.array)  #>mid count2 contain cvalue with maximum in  class \n",
    "            X2 = X[X[bstFtre]>=mid]\n",
    "            for j in range(3):\n",
    "                if count2 == root.array[j]:\n",
    "                    Y.iloc[X2.index] = j\n",
    "                    \n",
    "        return Y\n",
    "    \n",
    "    \n",
    "    mid = root.midF\n",
    "    bstFtre = root.f_to_split\n",
    "    X1 = X[X[bstFtre]<mid]\n",
    "    # here we pass our next data for the next level were data value is less than mid \n",
    "    predict(X1,root.children[0],Y,0)\n",
    "    X2 = X[X[bstFtre]>=mid]\n",
    "    # here we pass our next data for the next level were data value is more than mid \n",
    "    predict(X2,root.children[1],Y,1)\n",
    "    \n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this the fuction which compare our predicted data and actual data how accurate is our prediction\n",
    "def accuracy(root,x_test,y_test,Y,val):\n",
    "    y_pred = predict(x_test,root,Y,val)\n",
    "    return np.array(y_pred==y_test).sum()/len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level = 0\n",
      "Count of  0 = 37\n",
      "Count of  1 = 34\n",
      "Count of  2 = 41\n",
      "Current Entropy is = 0.6344312705534911\n",
      "Splitting on feature pw with gain ratio = 1.0\n",
      "\n",
      "Level = 1\n",
      "Count of  0 = 37\n",
      "Count of  1 = 0\n",
      "Count of  2 = 0\n",
      "Current Entropy = 0.6344312705534911\n",
      "Reached the LEAF NODE\n",
      "\n",
      "Level = 1\n",
      "Count of  0 = 0\n",
      "Count of  1 = 34\n",
      "Count of  2 = 41\n",
      "Current Entropy is = 0.4210281233360511\n",
      "Splitting on feature pl with gain ratio = 0.6255881494700155\n",
      "\n",
      "Level = 2\n",
      "Count of  0 = 0\n",
      "Count of  1 = 29\n",
      "Count of  2 = 1\n",
      "Current Entropy is = 0.09993493397123412\n",
      "Splitting on feature sl with gain ratio = 0.40801422054726605\n",
      "\n",
      "Level = 3\n",
      "Count of  0 = 0\n",
      "Count of  1 = 1\n",
      "Count of  2 = 1\n",
      "Current Entropy is = 0.6931471805599453\n",
      "Splitting on feature sw with gain ratio = 1.0\n",
      "\n",
      "Level = 4\n",
      "Count of  0 = 0\n",
      "Count of  1 = 1\n",
      "Count of  2 = 0\n",
      "Current Entropy = 0.6931471805599453\n",
      "Reached the LEAF NODE\n",
      "\n",
      "Level = 4\n",
      "Count of  0 = 0\n",
      "Count of  1 = 0\n",
      "Count of  2 = 1\n",
      "Current Entropy = 0.6931471805599453\n",
      "Reached the LEAF NODE\n",
      "\n",
      "Level = 3\n",
      "Count of  0 = 0\n",
      "Count of  1 = 28\n",
      "Count of  2 = 0\n",
      "Current Entropy = 0.09993493397123412\n",
      "Reached the LEAF NODE\n",
      "\n",
      "Level = 2\n",
      "Count of  0 = 0\n",
      "Count of  1 = 5\n",
      "Count of  2 = 40\n",
      "Current Entropy = 0.4210281233360511\n",
      "Reached the LEAF NODE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this the fuction where we call our decision tree \n",
    "decision_tree(x_train,y_train,features,0,0,root,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "# Y is the array which contain all zeros we pass it so that predicted value can be saved in it \n",
    "Y = np.zeros(y_test.shape[0],dtype=int)\n",
    "Y = pd.DataFrame(Y)\n",
    "# this print us the accurracy how accurrate is our formula \n",
    "print(accuracy(root,x_test,y_test,Y,-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
